Course Overview
Course Overview

[Autogenerated] Hello, everyone. My name is David Tucker and welcome to the course introduction to security and architecture on eight of us. I'm a cloud consultant and I help organizations plan, build and implement custom software solutions in the cloud I have over 15 years of experience in software, architecture and development, and during that time, few things have fundamentally changed computing like the cloud eight of us certifications add value for both technical and nontechnical. Resource is in today's digital world. In this course, I will walk you through an introduction to both security on eight of us, as well as howto architect solutions on the platform. This information is valuable for anyone using eight of us, but it is essential for anyone taking the eight of US certified Cloud Practitioner exam. First, I will review three core concepts that affect how you use the platform, including the eight of us well architected framework, the shared responsibility model and the acceptable use policy. Next, I will review security and user management on AWS, and then I will cover key architectural concept around fault tolerance, high availability and disaster recovery. Finally, I will review concepts around scalable and secure applications on Amazon. Easy to when you finish this course, you will have a clear understanding of these concepts that are needed for a portion of the AWS certified Cloud practitioner exam, as well as the knowledge needed to begin implementing solutions in the cloud.
AWS Architecture Core Concepts
Overview

[Autogenerated] so welcome to the third and final course on this path to help you on your journey to become an eight of US certified cloud practitioner. In this course, we're gonna be covering an introduction to security and architecture on eight of us. Now, up to this point, we have covered concepts like understanding the cloud as well as understanding eight of us. Core service is, But now we're gonna be implementing many of those things within the cloud through this course. So we'll be covering items like security where we need to know what is our responsibility when we're building a solution on eight of us, And what does eight of us provide out of the box for us? We're also gonna be looking at concepts like architecture and understanding how we build highly available and fault tolerant applications. So once you get through with this course, you will have the information you need on your journey to becoming a certified cloud practitioner. Now, the last module in this course will give you the details on how you prepare for sign up for an ultimately take your certified cloud practitioner exam. Now be sure to follow along with the study guides that we have provided. Be sure to print out that guided outline and keep it with you. Justus. You did in the other two courses in this path. Be sure to add notes as you go through each clip. In addition, note. The additional list of eight of US service is that we're going to be covering within this course and be sure that you're studying that before you go in and take the exam.
Security and Architecture Overview

[Autogenerated] Since we're covering security and architecture in this module, we want to start off with just a simple overview of what we're gonna be covering as well as a few basic concepts. So first of all, here's what we'll be covering over this initial module. First of all, we will be reviewing these core concepts around security and architecture. We'll also be exploring the eight of us shared responsibility model, which is a critical piece toe. Understand? If you're gonna be using the platform, well, then be introducing the eight of us. Well, architected framework. We will then be examining fault, tolerance and high availability on AWS and finally will be understanding that provided tools for compliance. So first of all, let's look a concept that governs how we use eight of us, and that is the acceptable use policy. And this is eight of us is policy for both acceptable and unacceptable uses of their platform, and for you to be ableto have an account on eight of us. You need to agree to abide by this policy. Now, this policy covers several things, including some simple things that are prohibited, for example, sending unsolicited mass e mails. That's not something they want you to do on their platform. And if they were to detect that you were doing that, they could completely close your account. Also hosting we're distributing harmful content, things like viruses and malware. That's something that's also prohibited now one of the things that has changed within the acceptable use policy. There are certain things you previously couldn't do without eight of US permission that you can now do. No one example of that is ___________ testing, and that's a type of testing that lets you see potentially where different holes are within your security. For example, you can see what ports are open on a specific server. It used to be you couldn't do that without talking to eight of us. But now they have provided a list of service is where you are able to do this type of testing, and as long as the service you're wanting to test is in that list, you can do that without permission. But let's talk about one of the core security concepts that we need to understand when using eight of us, and that is the concept of least privilege, access and the core of this concept is simply that when you're giving somebody permission to access eight of us, resource is within one of your accounts. You should, on Lee, grant them the minimum permissions needed to complete their tasks. And no more that you might say, Well, this applies to other people will. One of the ways that you can even apply this to the work that you do on the accounts that you own is that eight of us recommends that you don't use your route account as your day today account. There are specific things that the route account can do that no other account can do, so they recommend that you set up in. I am account to use on a daily basis, and this is just another example of least privileged access. But in terms of understanding how this plays out if you're at a company, for example, and you have several employees and maybe one employee needs to have access to a majority of the systems. But another employee is only doing a maintenance task, and on Lee needs access to those two systems will you need to make sure that that employees on Lee has access to those two systems. While it would be easier to give everyone the same access, we want to be sure that we're following least privilege access and on Lee give them access to what they need to do the jobs that they have been given.
Shared Responsibility Model

[Autogenerated] next, we're going to talk about the eight of us shared responsibility model, and this is something that everyone that uses the platform should understand. At a detailed level, eight of us puts it this way. Security and compliance is a shared responsibility between eight of us and the customer. But since it is a shared responsibility, we need to have a good level of understanding of what is the responsibility of AWS. And what is our responsibility as the customer at a high level, we can put it this way. AWS is responsible for the security of the cloud, and the customer is responsible for security in the cloud. So eight of us has a responsibility for those core system that is running the entire platform. But the customer has control over the things that they're putting onto the platform and how they're using it. But let's look at this at a more granular level of detail. So, first of all, for eight of us, they have the responsibility for doing things like access control and training for their employees. So they control what employees can access, what things and then make sure that they have the training to know what they need to do. As a part of that, AWS is responsible for those global data centers and the underlying network. So when we looked at the global eight of US infrastructure, they're responsible for those different availability zones and those regions and making sure that all of the connective ity exists between those. They're also responsible for the hardware for global infrastructure. So they're gonna take care of replacing servers and switches and all the other bits of networking gear that they have. They also a responsible for configuration management for the infrastructure. So determining how bits of data get from one location to another, they control all of that. And they also handle patching of the cloud infrastructure. And service is so the core servers, the bare metal servers that are actually running some of your virtual servers or the servers that are running many of the service's you use on AWS. They're in charge of patching those bits of cloud infrastructure. But now let's turn and look at what is our responsibility. So first of all, it is our responsibility for individual access to cloud resource is and training, so we need to make sure that we give least privileged access to those people in our company that need to access. Cloud resource is and were then also responsible for training them to make sure that they know how to use the service is that are available on eight of us. Another key one and one of the ones that has missed the most often is the customer is ultimately responsible for data security and encryption, and we say here both in transit meaning data going back and forth between different service's or different locations and data at rest. So data that is actually stored somewhere it is our responsibility to be sure that were following best practices there. So, for example, you could choose to put unencrypted data in s3. However, you also have the ability to configure the service to store that data so it is encrypted at rest. That is your responsibility. In addition, the operating system network and firewall configuration that is on you as the customer. So if you're using infrastructure as a service, so let's say you're using just E. C to virtual servers. You're responsible for that operating system, including patching if you're configuring your own. VP sees you're responsible for that network configuration and things like your access control list and security groups. You're also responsible for all the code that you deploy onto the cloud infrastructure. Obviously, there's no way eight of us can be fully responsible and own all of the code that you upload. So that is on you as the customer, and I kind of alluded to this earlier. You're also responsible for patching any guest OS and custom applications that you are leveraging. So it's important to note here, when you think of any solution that you're deploying on the cloud, that you clearly understand what is the responsibility of eight of us and what is the responsibility of the customer. And as you prepare for your certified cloud practitioner exam, it will be important that you can look at scenarios and understand who has responsibility in those scenarios.
AWS Well-architected Framework

[Autogenerated] If you're new to eight of us, it can be challenging to know how to best create solutions on the platform. And over time, eight of us has collected best practices that you should follow in these cases. And they have put these together into a single resource called the eight of us. Well, architected framework. And this framework is a collection of best practices that are organized across five key pillars that help you know how to best create systems that drive business value. So let's look at these different pillars. First of all, we have operational excellence. This is what helps us know that we're both running as well as monitoring our systems for business value. We want to be sure that we're being efficient with the time it takes to both build and deploy our solutions on to the cloud. Then we're gonna look at security, and this has to do with how we protect information and business assets, not Justus. We deploy them to the cloud, but also how we monitor to ensure that we're following the same standards of security throughout the life of those assets. Then we're gonna be looking at reliability and later within this module, we're gonna be covering the concepts of high availability and fault tolerance. And both of those fall under this overall pillar of reliability. This helps us make sure that our systems are up and running as often as possible. And next we have performance efficiency, and this is how we can make sure that we are leveraging. The resource is that we have spun up on eight of us and using only the amount that are needed to perform the tasks that we have for them. But we also want to be concerned with cost, and that takes us to the fifth pillar of cost optimization. We want to be sure that we're not paying any more than we need to to achieve the level of business value that we need. And this is going to cover concepts like reserved instances or spot instances or different s3 storage classes that we have already discussed. Now eight of us has collected this information on a microsite aboutthe well-architected framework. And so, as you go to get ready to build your custom solutions on the cloud, you can go review this resource to know how to best follow the recommendations within these five pillars.
High-availability and Fault Tolerance

[Autogenerated] So next we're gonna talk about high availability and fault tolerance, and both of these fall under the reliability pillar of the well architected framework. And the quote that should be our driving force, as we look at this concept comes from the CTO of Amazon, and he says that everything fails all the time. So if we start to build anything, even when we're looking at a cloud platform that has many capabilities, we need to be sure that we're building knowing that failure can happen at any point in our architecture. So let's look at the concept of reliability on eight of us, and we're really gonna look at two different concepts. The first is fault tolerance. This means that we're able to support the failure of components within your architecture. And if you remember, we have looked at this already. When we've looked at service is like sqs, and we looked at what we could do in using a cue to help mitigate challenges that would exist if some of our processing power we're actually to go offline downstream from the Q. Then we also have another concept, and that's an overall concept of high availability. This means that we want to keep our entire solution up and running in its expected manner despite any issues that may occur. So let's look at how we build solutions on AWS with these things in mind. First of all, most managed eight of US Service's provide high availability out of the box. And this is great because this limits the amount that we have to build. For example, if we're storing data using the S three standard storage class with an S three, that data is going to be stored across multiple availability zones. So we don't have to worry about building in a type of backup system for our production applications that's already baked into the service. However, when we're building solutions directly on E C two, for example, vault tolerance must be architected. We have to figure out where to build that in, and this goes back to our different cloud deployment models. If we're looking at solutions that our infrastructure as a service, this is where we need to really consider fault tolerance, because we're gonna have to figure out how to build it into our custom solutions, and at a minimum, we know that we need to integrate with multiple availability zones so that we can deal with the potential failure of a complete availability zone within a region. Now, some service's can enable fault tolerance in your custom applications, so there's a few to remember the first I alluded to earlier. And that is simple Q service or sqs. We can build cues so that data or events that need to be processed can be held in the queue until they are ready to be processed. That way, if anything downstream goes off line, we can still have a functional application. But in addition, we also have Route 53 we've discussed earlier within this path about how you can use Route 53 to detect if there are in points that are unhealthy and then route users to the right service is that are available
Compliance

[Autogenerated] So next we're gonna talk about the concept of compliance and depending on what your experiences within the technical world, this might or might not be a familiar concept to you. But let's talk a high level about some common different compliance standards. We have some standards like P C. I. D. S s. And this is a standard for processing credit cards. So a part of the agreement that you'll have with the credit card companies for being able to process those cards if you're dealing with the data directly, is that you only to meet certain standards. This means you need to be in compliance with P. C. I. D. S s. But we also have other standards, like HIPPA. And you might have heard of this one during some of your trips to the doctor's office, especially if you're in the United States. This is a compliance standard for healthcare data in terms of privacy, making sure that only certain people have access to certain pieces of data. Then we also have some more technical standards, like sock one, sock to and sock three, and these air reviews of operational processes related to your data centers. There also are some that are focused on government standards like Fed Ramp. And this is standards for us government data handling. And then we have others that air standards around things like personally identifiable information with I s 0 27 0 18 Now, these air different compliance standards and they're our service is on AWS that can help you know how to navigate these compliance standards. So we're gonna look at three in particular. First of all, we're going to look at eight of us config, which we have already mentioned within this path. And there are some conformance packs that are provided to help you with aspects of compliance. In addition, we have eight of us artifact, which provides self service access to eight of US compliance reports. And then we have eight of us guard duty, which provides intelligent threat detection so you can help monitor and try to detect if there are scenarios that are happening that are unusual, that could lead to you being out of compliance. So let's look here at what we're gonna cover over the course of this demo. First of all, we're going to examine how you get access to compliance reports utilizing eight of US artifact and then we're also going to be looking at the concept of conformance packs within eight of us. Config. So I'm here in the eight of US console and the first thing I'm going to do is I'm going to navigate to config. So all search for config, we'll launch it. And I have already gone through the process of setting up config here within this account. Now, in this point, we can go in and add rules directly. Or we could go in and just go under conformance packs. Now, here in going under conformance packs, we can simply go to deploy conformance pack. And here, If we knew that we were going to be processing credit card information, we could look at the sample template for operational best practices for P. C I. D. S s. And if we chose to deploy this template, this would help us monitor whether or not we were in conformance with P C I. D. S s standards. Now, the next service we're going to look at is going to be eight of us artifact, and you can see here. This isn't my recently visited service is so I could search for it or I could just simply click on the name. Now from within here, we're going to be able to get access to two different pieces of information. The first is going to be eight of us specific compliance reports and the second are going to be any specific agreements that we have in place with eight of us. For example, if we were processing hip, a compliant data, we would need to have a B A agreement in place with Amazon, and we would be able to find that here But here within eight of US artifact, we could go in and search for information around specific compliance standards that we were interested in. And in some cases we would be required to provide this information to other sources, whether their governmental or just regulatory agencies for specific areas. And so if we needed to find a specific standard, we could actually scroll through this list, find the right standard, and then we could actually get the artifact. Now, for many of these artifacts, we will need to sign a non disclosure agreement with eight of us to get access to that, that this provides a self service way to get access to these compliance reports that are needed
Scenario Based Review

[Autogenerated] So next we're gonna talk through some samples scenarios to see how well you have done at absorbing the material that has been presented within this module. And first we're gonna be talking about Jane and her company is building an application, and they will be processing credit cards and they're gonna be processing those cards directly and not through 1/3 party service. And so the bank that they're working with needs a PC idea says compliance report for AWS. So for Jane, where should she go to get this information? Okay, next, we're gonna be looking at Tim, and Tim's company is considering a transition into the cloud, and they do store some personal information, but they stored securely within their systems. And so Tim CTO has asked him what the company's responsibility is for security on this once. This is transition to eight of us. So if you were in Tim's roll, what would you tell his CTO? Okay, Next, we're gonna be talking about Ellen, and she is a solutions architect at a start up, and they're building a new tool for digital asset management. And she's curious how to best leverage the capabilities of eight of us in this application because they haven't built any applications in the cloud yet as a company. So what resource would you recommend for Alan and her team to review? So in the next clip, we're gonna run through a quick summary of what we've covered in this module, and then we'll run through the answers to these scenarios.
Summary

[Autogenerated] So we've covered quite a bit in terms of introducing security and architecture on AWS here within this initial module. And so let's quickly take a look back at what we've covered. First, we reviewed the core concepts around security and architecture. We also explored the eight of us shared responsibility model. So we had a clear understanding of what is our responsibility as the customer and what is eight of US responsibility. When we're leveraging the cloud, we also introduced the eight of us well, architected framework. And we talked about the five different pillars that it provides for helping us know how to best take advantage of the platform. We then examined fault, tolerance and high availability in terms of concepts around reliability that we need to put in place with our custom solutions. And we also understood the provided tools for compliance. So now let's go back and let's review our three scenarios that we presented within the previous clip. And so first we had Jane and her company was looking to process credit cards and she needs to get a P. C. I. D. S s compliance report for eight of us. So where would she go Well, in this case, she would goto aws artifact. This service exists to be a self service solution for getting these types of compliance reports whether we need to give them to organizations like banks were working with or governmental agencies. Now the next scenario has to do with Tim, and Tim's company is considering a transition to the cloud. But the CTO has asked, what is the company's responsibility in terms of security? And so here. What should Tim tell his CT? Oh, well, I think he should tell him to review the shared responsibility model because in terms of this data that they're storing because they need to understand that concepts like data security and encryption are the responsibility of the customer and not the responsibility of the cloud provider. And finally we have Ellen and Ellen was curious how to best leverage the capabilities of eight of us for this custom application that is being built at her start up. So what would you recommend? Well, I believe in this case, the well-architected framework would be a great resource for Ellen and her team to review, because they're gonna be able to look at the five different pillars and understand how to best take advantage of those capabilities within the cloud
AWS Identities and User Management
Overview

[Autogenerated] So next we're gonna be talking about eight of US identities and how we handle user management on the platform. I quickly want to remind you of a term that we introduced in the last module, and that is least privileged access. And this has to do with granting permission for a user to access your eight of us resource is and granting them the minimum permissions needed to complete their tasks and no more. And we're going to see within the course of this module how you actually make this happen on eight of us. So over the course of this module, we will first be introducing a service called eight of US Identity and Access Management. Or I am. We will then be reviewing the different I am identity types. We will be enabling multi factor authentication, or M f A, and will also be introducing a service called Cognito that will enable you to take this kind of authentication and authorization to your own custom applications.
Introduction to AWS IAM

[Autogenerated] So next we're gonna be talking about the eight of us. I am service. So in a high level, this is the service that controls access to eight of us. Resource is so if you want to give someone within your company access to eight of us, let's say through the console, for example, and you want them to be able to spin up E C two servers, but nothing else. This would be the service where you would go and both create their user and also go in and configure what that user can do. Now this service is free. So, unlike many of the other, service is on eight of us. Where you pay based on the resource is you create within the service. This service is included at no charge to everyone that has an eight of US account, and it manages both authentication and authorization, and we kind of mentioned both of these earlier, but we didn't use the terms. Authentication is what verifies users for us and lets them log in and manages their credentials. For example, authorization is where we actually configure what that user can do. So we could say you have access to all easy to actions, but nothing else. For example, Now I am also supports a concept called Identity Federation, and this allows us to use an external identity provider toe actually handle that authentication portion of I am now. This might not make sense. If you have a company of three people, you can simply go in tow. I am, and you can create all of those users and manage their permissions. But if you have a company of 2000 resource is all of whom need to have access to eight of us. Using an external identity provider, especially if you already have one in use, would totally make sense Now. You don't need to know how identity Federation works for the exam, but you do need to know that this is an option when you're working within, I am next. Let's talk about the three different identity types that exist within I am, and the 1st 1 is just a user. And this isn't account for a single individual so that they can access eight of us. Resource is so let's say we have a new employee named Jane, and she is one of our developers and so we could go in, give Jane and account and then give her permissions to access specific eight of us. Resource is, however, let's say it's not just Jane. Let's say Jane is a part of a new development team that we're bringing on board and there are five developers. Well, we could easily go in and create an account for each of those and then go in for each of them and then add the exact same permissions on all five of them. However, there is a better approach. So another identity type within eight of us I am is a group, and a group allows you to manage permissions for a group of I am users, so you would still need to go in and create users for each of them. However, you could then go in and add them to a group and then assign permissions to that group. And when we're talking here about an I am identity were really just saying anything that you can assign permissions to and so we've talked about users. We've talked about groups. Now we have 1/3 type that's a little bit different, and that is a type called a role, and this enables us to either have a user or a service, assume permissions to perform a specific task. Now, we have run across this already, although we didn't call it out. So when you launch an E C to server, it has the option to specify a role for that server. So you could say that that server needs to have access to an S three bucket. For example. Maybe a part of the web application on that server is that it needs to upload photos to an S three bucket. Well, it can't do that by default. The permissions won't allow for that. However, you could give it a role and then give that role the permission of being able to write to that s three bucket. And this is what you can do utilizing roles within. I am next. Let's talk about how you assign permissions, and this is what leads us to the concept of policies within. I am So a policy is just a Jason document, and it defines permissions four and I am identity. So again, a user group or a roll. And it defines both. The service is that the identity can access and what actions can be taken on that service. So let's go back to the example that we mentioned earlier with R E. C to server. We might want tohave read access and right access to a specific S three bucket. But we wouldn't want it to have permissions for everything on s three because that would also include deleting the bucket. And that's not a part of what that Web application would need to do. So we can say that it can access the S three service for just the read and write actions on a specific bucket. Also, policies can be either customer managed or managed by eight of us. So let's look at the customer manage portion. So if you need custom policy needs, for example, in looking at our Web application, if we know that we want to give that Web application access to read and write to a specific bucket, we could write a custom policy to do that for us. But eight of us also provides what it called manage policies, and these policies have already been created. So if you wanna have something like read only access to an entire AWS account or if you wanna have something like full access to s three, there are policies that eight of us has already created and they maintain and you can choose to use those without creating your own custom policies. Now here is an example Policy, and this is what's on the left side. Now, this is written as a Jason object, and you can go in and have specific permissions. Now you can see here that we do have a statement here that is allowing, and in this case it is allowing all s3 actions on a bucket and on the contents of that bucket. Right below that, we have another statement and this is a statement denying access except for having access to that specific bucket and its contents. This is just an example. Policy now is a note. You will not need to know how to create a policy or any of the specific aspects of policies for the exam. But I did want to give you an example in terms of what one of these policies looks like. Now let's next talk about some different best practices when working within. I am now the first best practice is multi factor authentication, and what this does is this gives an extra layer of security for our users. So when they log in, they will log in with their user name and their password and then a token. And this is either going to be a physical or a virtual device that's gonna generate that token for logging. And you might have seen this in other places. Many Web service is now offer multi factor authentication, but this is just another layer of security. Then we also have least privilege access, which we have already talked about. So when we're creating and managing users within, I am, it is ideal that we encourage our users to set up multi factor authentication, and we configure permissions to be geared towards least privilege access.
Creating and Managing IAM Users

[Autogenerated] So now we're gonna begin working within the I am service and we're going to be creating and managing I am users. So over the course of this demo, we will first be creating a new I am user. We will then be configuring permissions for that I am user, and then we can go in and create an I am group and attach permissions to that specific I am group. So I'm here within the console and you'll see here. That I am is listed under my recently visited service is so I could either search for it. I could use the service is dropped down, but in this case, I'll just click on I am. Then I'll make sure that users is selected within the left navigation, and then I can go in and actually work toe. Add a new user so I'll select the option for Add User. Well, go ahead and give us a name. We'll say that Tom is a new developer on our team, and in this case we have a choice. We can choose to give Tom just programmatic access, and this is going to be with an access key and the secret access key, or we can choose to give Consul access or we can do both. In this case, we will just grant Tom access to the eight of US management console. And in this case, we could choose to have an auto generated password or a custom password, And we're gonna also set it so that Tom will have to reset his password when he logs in next to the console. So this has what we need. So we can now click the next option to set some permissions, and in this case, you can see that it gives us several choices. We can go in and add the user to a group, and we're not gonna do that just yet. We will do that later. Within this demo, we also can copy permissions from an existing user or we can attach existing policies directly. So I'm gonna go ahead and select this option. Now we have an option in here to look at the managed policies. You can see here that it's a manage policy when under type, you see that it is AWS managed. Now, in this case, we can select those, or we could choose to create a new policy using the create policy button. In this case, Tom needs to have access to s three. So let's Inter s3 and let's search for what policies are available. And we can see here that there are four different policies that show up, and they are all eight of us managed. Well, in this case, let's say that Tom is gonna manage all of Rs three buckets for the organization, but we probably wanna have him have s three full access so we can go ahead and attach that to his user. We could choose here to go in and add tags for this specific user. This could be valuable if you wanted to document different departments or different organizational structures. But we're not going to use this for the moment. And then we can go into review. Now, when we get to review here, we can see that we have the user name. We have the different settings that we put in place, as well as the specific policies that were added in. Now you'll notice here that it also has another policy for I am user change password because we selected the option to force him to reset his password, it automatically added this manage policy to his user. Once we have all of that in place, we can simply hit, create User. Now, While this was relatively easy for us to put into place, this would become problematic if we were trying to enter in a bunch of users and trying to configure specific permissions for those users, especially if we weren't creating them all at the same time. So in this case, I'm gonna go ahead and I'm going to click close. And from here, we're going to navigate to the group's option. And here we're going to create a new group. So here will call this deaf team one is gonna be our group name. And so, from here, we're going to go in and add the Amazon s three full access manage policy. Well, then hit next step. And now we can just create a group. There's not a lot of steps involved with creating a new group. Now, this group is currently in place, but it has no users attached to it. So now let's navigate back over to the Users Tab and from here, we're gonna go into the user that we previously created Tom Williams. First, we're going to go in and we're going to remove the Amazon s three full access manage policy because we're gonna choose to not manage that within the specific user. But instead, we're gonna manage it from the group. So we've now detached that policy. So as of right now, Tom has no access to s three. But then we're gonna go over to the group's tab. And from here we're going to select the option toe. Add User two groups. We're going to select the Deaf Team one group, and then we'll say, Add user two groups. Now it's important to note here that you can have users in multiple groups. That's part of the benefit. So if you had someone that was a member of deaf Team One, they could be a part of that specific group. Let's say they're also an architect and they're part of the Enterprise architecture group for the organization. You could create another group for that, and then you could add him to that group as well, and he would actually have the sum of both of the permissions from those groups. So now we've been able to walk through and create an I am user. We have configured permissions for that user and then created a group, added them to the group and then attached permissions to that eight of us I am group.
Enabling Multi-factor Authentication

[Autogenerated] So next we're gonna walk through the process of enabling multi factor authentication. Since this is one of the best practices for working within, I am. I want to be sure you have the tools to know how to do this. So, first of all, we will be enabling multi factor authentication for the route user. And then we'll talk about enabling multi factor authentication for and I am user because these two processes are different and you do them from different places. So let's dive in. So now I'm here in the management console. Now I have logged in as a route user, and it's important to note here that you can on Lee manage the multi factor authentication for the route user as the route user. So I'm going to first go under my name here on the top. I'll open up the drop down and I'll go to the option for my security credentials, and this is where you manage all of the information around security credentials for the route user. This is where you would go to change your password is the route user, or to update other things like your root access keys. Now I'm going to open the tab here for multi factor authentication. And I have an option here. Toe activate multi factor authentication. Now, I purposely went through on this and the I am account that I'm using for this demo and removed multi factor authentication before this demo, You should always have multi factor authentication enabled. It's just a best practice that you should follow, so I'll hit the option here to activate multi factor authentication. Now, you have several choices here. You have a virtual device. You have a you two f and you have other hardware tokens. For most of you, you should use the virtual M F A device option. This is what will let you use an application like Google Authenticator. Or you can use a password manager that has it built in like one password. So I'm going to go ahead and select. Continue, and you'll note here that it gives you a list of compatible applications and this list. Let's you know other things that you can use toe actually use multi factor authentication, and you can go through and see also the hardware keys that are supported. But you can see here if you're using iPhone or Android. We have solutions like Offi duo Mobile, last pass, Microsoft authenticator and Google Authenticator. So in this case, I'm going to go back over to the console. And I have a password manager that I use that all used to set this up. So the first step is gonna be I need to actually show this Q R code. Now, you also can use a secret key to make this work, but I'm just gonna go ahead and select the option to show the Q R code. Now, I will be deleting this multi factor authentication device after I finish this demo and then I'll be going back in and adding another one. So for security reasons, you wouldn't want anyone else to have access to this Q R code because then they would have access to be able to generate your secret keys. So I'm gonna go in and here with my password manager. It lets me actually just drag over the Q R code, and then it's now actually entered in. And so what they're gonna have you do is you're gonna type in a two consecutive codes, and this is important because they want to be sure that it has properly sink TTE. And so it's going to validate these two codes before this adds it to your device. Because once you add multi factor authentication to your device, you will have to log in with that. And if you need to disable that, if you lose access to the vice that has it on it, you will need to contact eight of US support to try and get access back to your account. So I'm going to go ahead and enter in the second code and then I'll hit a sign M f A. And now we can see that it has been added. And so I will have to enter in the secret code every time I actually log in to this route account. Next. Let's go look at how we do this for and I am user. So I'm going to choose the user, david dot Tucker, and from within here, I'm now going to go to the tab for security credentials and you can see here that there is not an assigned M f A device and so we'll go to manage. I'm gonna choose the same option for virtual M F A device. And just as before, we would go through the process, as we just did, to show the Q R code. Add that to our password manager and then enter into success of keys. And then once we did that, we would have it in place for I am user so they would have to enter in that token every time they logged in.
Amazon Cognito

[Autogenerated] So next let's talk about a new service, Amazon Cognito, and this service gives you the capability to manage authentication and aspects of authorization for your custom Web and mobile applications through eight of us. So let's go ahead and dive through this a bit more, and we'll talk about some use cases that tie in with it. So Amazon Cognito, first of all, is a fully managed user directory service for custom applications. So I am deals with permissions for eight of us. Resource is, but what happens if you basically want to build something like I am for your own custom applications? Amazon Cognito fills that role that it does more than just that. It also provides you I components from many platforms. So if you wanna have a sign in or sign up you why, for example, for your IOS application or for your react Web application of your android application, you can get those out of the box with cognito. It also provides some advanced security controls to control account access. There's actually some pretty advanced functionality baked into cognito that you can choose to take advantage of, but the other part of this and this is where it intersects with I am is it also enables you to control access to eight of us. Resource is let me give you an example. Let's take the example that we mentioned earlier within this module that let's say you have a Web application that lets users upload some of their photos, for example. Well, you would want to be sure that users have access to a part of a specific S three bucket that's just for them. Well, there's a way with cognito that you can configure access to specific pieces of eight of US infrastructure that you would want a user toe actually have access to, but without having to have them sign up for an I am user account. Now this can also work with social and enterprise identity providers. So we talked a little bit earlier about Identity Federation, and Cognito has wide support for that with providers like Google, for example. And let's be honest, most people seem to have a Google account, so you could let your users log into your custom application with Google and have that correspond to a cognito identity. But it also supports Amazon and Facebook and Microsoft active directory as well as any sample. 2.0 provider. And so cognito gives you the ability to take this level of authentication and authorization and tie it into your custom applications.
Scenario Based Review

[Autogenerated] So next we're gonna talk through some specific scenarios to see how well you've absorbed material that we've presented within this module. So first of all, we have Sylvia and she manages a team of Dev ops engineers for her company. Now, each member of that team needs to have the same access to cloud systems, and it's gonna take her a long time to attach permissions to each user for access. So if you've got a team of 10 and need to go through to each individual user and go in and add those permission, she's just saying, man, it's taking a really long time. So what approach would help Sylvia manage the team's overall permissions on eight of us? Okay, next we have Edward and he works for a startup that is building a mapping visualization tool, and their E C two servers need to access some specific data that's located within s three buckets. Now he created a user, and I am for these servers. And then he uploaded those keys directly to the server. So is Edward following best practices for this approach? And if not, what should he D'oh! Okay, Finally, we have William and he is leading the effort to transition his organization to the cloud. Now his C I. O. Is concerned about securing access to eight of us. Resource is with a password, and he's asking William to research approaches for additional security for their users. So what approach would you recommend to William for this additional level of security? Now, in the next clip, we're going to go through the answers to these scenarios as well as taking a quick look back at what we've covered within this module.
Summary

[Autogenerated] So we have covered a great deal with in this module and before we dive in and go look at our scenarios, let's take a minute to look back at what we've covered. First of all, we introduced the I Am or Identity and access management service, and as a part of that, we reviewed the three different I am identity types, users, groups and rolls. We then were able to go through the process and enable multi factor authentication. And we did that after we went through and learned how to create. I am users and groups within I am. And finally we introduced Amazon Cognito, a service that enables us to configure aspects of eight of US access for our custom applications and our users within those applications. So now let's take a look at our three scenarios. And first we had Sylvia and she was trying to figure out how to manage the team's permissions. So what would you recommend for her? Well, in this case, using an I am group would give her one location for setting permissions for every single member of the team. However, we do need to add a caveat. Here we have the principle of least privileged access. We want to be sure that no one has access to more than what they need. So for the members of Sylvia's team, it totally makes sense. Tohave an I am group. But if she simply put all of her technical resource is in a single group and gave them all admin access, that would not be following least privilege access. So we do need to use a group here, but we want to be sure we're not giving them more access than what they need. Next we have Edward and he has e C two servers that need to have access to s three buckets. And so he just created a user in I am and then uploaded that access key and secret access key to the server directly. So is he following best practices? No, he's not following best practices here. And part of the reason I bring this scenario up is even this week. I was working with the client and I had pretty much this exact scenario happened where I found out the client was creating a user for something and upload in that directly to the server. The solution here is to use, and I am role within Easy to because this mitigates much of the security risk as opposed to taking the approach that Edward is taking. So roles are around for just this reason. To give our service is the ability to work with other service is on eight of us, but to have it happen in a secure way. So Edward should transition away from his approach and move to using an I M. Roll with specific permissions in this case to Access s three. Finally, we have William and he's leading the effort to transition his organization to the cloud, but he needs an extra level of security. So what approach would you recommend to William? Well, in this case, multi factor authentication should be what they move towards this is built into AWS, and because of that, it's something that should be relatively easy for them to implement. And it moves from the user simply having to just have a password which, obviously we know that passwords can be either hacked or they could be stolen from other service is, and especially if users are using the same password on multiple sites, it can expose organizations to risk, but with multi factor authentication, you have to be able to provide this other token to be ableto log in. So this should give William the security that he needs.
Data Architecture on AWS
Overview

[Autogenerated] since we're talking about architecture on eight of us. Within this course, one of the areas that we need to touch on has to do with data. So let's talk about what we're gonna cover within this module. So first of all, we're gonna look at some different approaches for how you integrate data from your own data center. Now, this is different than what we talked about earlier. With just data transfer service is we're gonna talk about how you integrate the data you have within your data center with experiences on eight of us. We're also gonna be examining approaches for how you process your data. Because in so many architectures, there's a processing step that needs to happen with data before you can dive in and fully analyze it. But then we'll be looking at the different data Analysis approaches, and service is that you can leverage on AWS. And then finally, we're gonna talk about an exciting topic, and that's how you integrate machine learning and a I. Those capabilities that eight of us has integrated into the platform into how you analyze your own data.
Integrating On-premise Data

[Autogenerated] So first we'll be looking at how you integrate your on premise data into eight of us. And really, we're gonna be talking about two different solutions. One of them is eight of us Storage Gateway. And this is a hybrid cloud storage service. So merging together the storage you have between your data center and eight of us and then we have eight of US Data Sync, and this is an automated data transfer service so different than snowball. But here we're talking about how you actually can set up a system from your network to sink data with AWS. So first of all, we have eight of us storage Gateway, and this integrates cloud storage into your local network. And the way this works is you can deploy either a virtual machine or a specific hardware appliance running the storage gateway software on it onto your network, and it integrates with s3 and E B s. Now with in Storage Gateway, there are three different gateway types that you can leverage. One of them is a tape gateway. Then we have a volume gateway and finally ah, file Gateway. So let's look at those three different gateway types So, first of all, let's look at the file Gateway. This enables you to store files within Amazon s three, But yet keep certain files local to your network on the storage gateway device so that you can have low latent see local access, and it can have this local cash pull certain files or files that are the most recently accessed. Then we have the tape gateway, and this enables you to have basically a tape backup experience. So if you don't have a lot of historical, I t background. One of the ways that data is backed up, especially within the enterprise, is on tape devices, and you can end up with a tape library where you have a machine that's able to read data and write data to multiple tapes. Well, the tape gateway for storage Gateway basically acts like a virtual tape library or V t l. And so you can have that same backup experience still using your same backup software, but have it store to the cloud where it's much more durable than actually storing on tapes. Then we have volume gateway, and this gives you the ability to have ice scuzzy volumes that are needed by certain local applications, but have that data actually stored in the cloud. So next let's talk about eight of US data Sync. Now, Data Sync works by having the data sync. Agent deployed is a virtual machine on your network and in this case, we have wider support than just with the previous approach. With storage Gateway here, we can integrate with S three E f s and F S X for Windows file server. We also hear see a greatly improved speed of transfer due to this custom protocol that eight of us has developed and different optimization, and in this case, you are charged per gigabyte of data transferred between your data center and eight of us.
Processing Data

[Autogenerated] So next we're going to cover how you process data on eight of us. And the different service is that are provided for you to do this because in many cases, if we're looking at how organizations analyze their data, there's a step for converting and preparing data before you actually analyze it. And that's what we're covering here. And we're really gonna be talking about three different service is first of all, eight of us and glue. Now, this is a managed extract, transform and load service. Now we're gonna be talking about extract, transform and load quite a bit. And so we'll call this E T. L within this clip. But let's talk about what that means. Usually it means you have data stored somewhere you need to pull it out of where it's stored. That's the extract. Then you have transformed. This means you're going to do things like normalizing the data. Maybe you need to change the way that phone numbers air represented or group things together or change the structure of the data. You need to do something to change it. Then we have the load step. This means you're gonna put it in a new location so that you can then go in and analyze it. So that's what we mean by extract, transform and load. Then we have another service, and that is Amazon E M. R. Or elastic map reduce. And this is a big data cloud processing sweet, and it uses popular tools, and we'll cover what those tools are in just a minute. And then finally, we have eight of US data pipeline, and this is a workflow orchestration service across separate eight of US. Service is so if you want to manage the process of how data gets from point A to point B, especially if it needs to go through a specific type of eight of US service in the middle. This is where data pipeline could be very, very helpful. So let's look at each of these in turn. First of all, we have eight of us glue and has mentioned it is a fully managed E T. L service on AWS. It also supports Amazon RD s dynamodb red shift and s3. So if you're looking for the eight of us service is that it integrates with those are the ones that you can look at Now it supports a server lis model of execution. That means you don't need to spin up specific servers or specific instances. You just use the service and it handles the management of the infrastructure for you. Next, we're gonna talk about Amazon E M. R. Or elastic map produce. And what this does is this enables big data processing on Amazon easy to and s three. And it supports popular open source frameworks and tools, which we will cover in just a bit. And it operates in a cluster environment without additional configuration. And this is really one of the benefits of the service. If you were choosing to do this on your own, just directly in Amazon, easy to there would be a lot of configuration that would go with getting these tools set up and then having them operate in environments where you have more than one server where they all need to work together. And this is just included as a part of the service and because it includes these popular tools and it has this clustered environment, it supports many different big data use cases. So let's look at the different frameworks that air supported First of all, we have Apache Spark Apache Flink. We have Apache hive Apache hoodie. We have H base and presto. So if you look around in terms of what people are doing within big data, generally these are the tools that are involved. So EMR gives you the ability to take advantage of these tools without having to spend a massive amount of time configuring them. Next, we have data pipeline and data Pipeline is also a managed E T L service on eight of us. Now it does manage the data work flow through eight of US Service's, and it supports S three e m r, red Shift, dynamodb and R. D s. And they're also our ways with this where it can integrate your on premise and data stores within your data pipeline.
Analyzing Data

[Autogenerated] So now that we've talked about how we get our data to eight of us and how we process it, we're now gonna talk about how you actually analyze the data that you have within AWS. And so we're gonna be talking about several. Service is, first of all, we're gonna be talking about Amazon Athena, which this is a service that lets you query data not stored in the database but actually stored in Amazon s three. Then we're gonna look at Amazon Quick site, which is a WSB I tool that gives you interactive dashboards. And then we'll be talking about Amazon Cloud Search. And this is a search service that can be used for custom applications. So let's dive in and look at these. So first of all, we have Athena and this is a fully managed server lis service. So you don't have to configure any of the underlying infrastructure. You can simply leverage the service. And the great thing about Athena is it doesn't able you to query large scale data stored within Amazon s three. So if you're taking more of what we would call a data lake approach and your company is just dropping a bunch of data within Amazon s three. Now, there are four mats that you can store it to make this process more efficient. But as long as you have that data there and you wanna run a query to determine your sales year over year, you could do that. Utilizing Amazon Athena. You also can write queries just using standard sequel. So this becomes advantageous for people that are used to working within databases. They can take that exact knowledge and use it to then query this data stored within s three. Now, with Athena, you are charged based on the data that you have scanned for a query. So depending on the amount of data that it needs to search through, that's how you are charged for Athena. So next let's talk about Amazon quicksight. And this is a fully managed business intelligence. Or sometimes you'll hear it called B I service, and this enables some dynamic data dashboards that are based on data that you have stored within eight of us. Now there are a couple of different pricing models, depending on how you're leveraging it. So we do have a per user and a per session pricing model, and it depends on how you're using it. How many authors you need versus how many people that are just reading the data. And there are multiple versions provided based on your needs. So, like a standard version versus an enterprise version. And they have different capabilities and different cost points, and so you could dive into the documentation and see which of those fits your specific needs. So here you can see an example from QUICKSIGHT. Now, this is using a sample data set that you can load into the platform once you have an account. And this gives you the view of an author who's able to go in and actually create new visualizations based on data that you have stored within eight of us. Now, next we're gonna look at Amazon Cloud Search, and this is a fully managed search service on eight of us. So in some cases you want to go through an analyze data in dashboards like we were talking about with quicksight, Or in some cases, you might want to run through a bunch of large scale data like we have with Athena. But maybe you want to build a custom application and make a lot of data available to your users. And this is where cloud surge could be very beneficial. The great thing is, it does support scaling of search infrastructure to meet your demand so you can trust that this is a service that can scale for you. Now you are charged per hour and instance type of your search infrastructure. So this is where you need to know what size of an instance you need tohave, and then you will be charged based on that. No, it does enable developers to integrate search into their custom applications. That's the goal of this service. So if you want to be ableto have users search through, you know a Thanh of PDF documents that your organization has stored or other custom information. This becomes a great resource to plug into
 Integrating AI and Machine Learning

[Autogenerated] So next we're gonna talk about the exciting world of a I and machine learning on AWS because now that you have data that you have included into the platform and you've processed that you've analyzed it, there's a lot you can do with that data and a I and machine learning give us capabilities that we did not have available to us previously for analyzing large amounts of data. So let's look at some of the different service is now. I do want to call out here we're not covering all of the service is on AWS. I'm simply covering a cross section of the service's things that you will need to know for your certified cloud practitioner exam. But there are many more. Service is beyond what I'm covering here. So we're really gonna be looking at three different service is the first is Amazon recognition. And this is a computer vision service that is powered by machine learning. And by this, we mean we can get insights out of images that we have stored on the platform. Then we're gonna look at Amazon translate, and this is what allows us to translate tax from one language to another. And finally we're going to look at Amazon transcribe, which is a speech to text solution, and this is also using machine learning. So let's look at each of these. First of all, we have Amazon recognition, and this is a fully managed image and video recognition deep learning service. Now that's a mouthful. But what this is is it means we can pull data and insights out of both images and video, and it can identify objects within images. So if you want to try to just quickly detect within an image, what objects air there, you can simply pass the image into recognition, and then you'll get a response back that contains those images. However, it does get interesting when we're looking at videos because you can identify objects and actions. So there are certain things that we wouldn't know if we only looked at a still image. But when we can see it in motion, we can begin to detect actions that are happening within that video. Now, another part of this is that it can detect specific people using facial analysis so you can go in and say here's an image of an individual and then you could have it go through a bunch of other images and see if it finds that person within those images. Now this falls under the category of facial recognition, and obviously people have a lot of varied opinions about facial recognition. But if you're looking to implement something like that or a custom image based authentication system, you could use Amazon recognition for that now. It also supports custom labels for your business objects. So let's say you had a store, a retail store and you wanted to take pictures of your products, and then you wanted to be able to detect when people are checking out, just based on a picture of their shopping carts. The items that they had and we've seen Amazon implements similar technology. With their go stores, you could use custom labels for your business objects. There's a way to train the system to be able to detect those custom objects. Okay, next, let's talk about Amazon translate, and this is also a fully managed service and in this case, were translating from one language to another, and it currently supports 54 different languages. It also has some cool features, like just language identification. So it's possible that you're receiving information and you don't even know what the source languages so you can get that from this service is well, and it also can work both in batch and in real time. So in terms of batch, we mean we have a bunch of text after the fact, and we want to get it translated. And real time means were actually able to stream that in and get the translation back as we're streaming it in now. We also have Amazon transcribe, and this is a fully manage speech recognition service. This means we want to take audio either again through real time or in a batch mode, and we want to be able to get that converted into text so you can have this where you do record speech and then converted into text in your custom applications. And one of the great things about this is there is a specific sub service here that is just for medical use. So if you're working within the medical area and you want to be able to transcribe information, that's gonna have medical terms in it, there is a part of this service for that now. This also supports both batch and real time transcription, and it currently supports 31 different languages globally.
Scenario Based Review

[Autogenerated] So next we're gonna dive through and look at some scenarios to help. You know how well you have absorbed the material within this module. So, first of all, here we have Ruth and she is a data scientist for a financial service is company, and they need to be able to process a large scale data set before they're able to analyze it. Now, Ruth doesn't want to manage servers. She really just wants to define how the data needs to be transformed. So if that was the case, what service would you recommend to Ruth? Okay, next we have Jesse and Jesse is a member of the I T. Team for a biotech company, and she's currently working to identify an approach for controlled lab access. So in other words, they need to be sure that that lab stays locked except to the people that need to be in there. Now she wants to leverage a I to determine access based on facial imaging. So they have a camera poster there. She wants to be able to just scan somebody's face and determine if they need to be in the lab. So is there an AWS service that can help with this approach. And finally we have Roger and his company sells custom service is around machine learning. Now his head of sales is trying to find a great way to visualize their sales data because they're tired of having to always go to the engineers to get this data. Now their data is currently stored within red shift. And so what they're doing is they're trying to figure out a way that they can have a self service experience for their salespeople to get access to that data. So what eight of US service would allow this access to the data by non technical resources that will cover the answers to these scenarios, as well as a review of what we've covered within this module within the next clip?
Summary

[Autogenerated] So we have covered quite a bit within this module around the topic of data and data architecture. So let's quickly take a step back and review what we've covered. So first of all, we reviewed some different approaches for integrating data from your own data center. Then we examined approaches for processing data, and we looked at concepts like E T. L in the different service is that can support it. We then explored some different data analysis approaches everything from using Athena to look at the data and s3 to diving into using B I dashboards with quicksight as well as even looking at creating a custom search solution using cloud search. And then after that, we integrated machine learning and a I into our data analysis with service is like translate and transcribe an image in video analysis with recognition. So now let's dive in and look at our scenarios. First of all, we have Ruth and she was looking to find a way to process data without having to manage the underlying servers. So what would you recommend? Well, I think in this case, eight of us glue becomes a great choice. It gives her that e T l capability, but in a server lis way without having to manage any of the underlying infrastructure. Then we have Jesse and she is looking here to use a I to determine access to the lab based on facial imaging. So what would she use? Well, in this case, Amazon recognition becomes a great choice because there are ways within recognition to store the images of specific people and then detect those in other images so they could integrate that into the camera that they have posted for lab access. Next we have Roger and he was looking for a way to have their sales team get access to sales data, but in a way where they didn't have to go in inquiry a database. So when an interactive dashboard, So what eight of US service would allow this access to the data by non technical resource is well, in this case, that would be Amazon quicksight. This gives a great way to create some self service dashboards based on the data that's stored in red Shift
Disaster Recovery on AWS
Overview

[Autogenerated] So next we're gonna be talking about disaster recovery on eight of us. Now, when we talk about disaster recovery according to the AWS definition, we're talking about how we prepare and recover from the disaster, which can be any event that has a negative impact on your company's business. So this could include things like hardware or software failures. Network outage is power outages, physical damage to a building like fire, flooding, human air or some other significant event. Now let's look at this from two different perspectives. First of all, let's say that you are a company that has your own data center. And if that data center is located within your town and there is a major event that happens, let's say an earthquake, for example, and your network connectivity gets disrupted. How do you handle that? That's something you need to prepare for. But they're also is another scenario. Let's say that you're leveraging the cloud for your infrastructure and let's say you're using AWS and you're basing everything in one of the eight of us regions now, while a complete region outages pretty much unheard of at this point, it certainly could happen, especially if there were a significant natural disaster. So how do you prepare there? How can you leverage the eight of us Global infrastructure toe limit the effect of this disaster. So over the course of this module, we're going to be talking about several things. First of all, will be helping toe understand the need for a disaster recovery strategy, and then we'll be reviewing before different disaster recovery approaches that are recommended by eight of us. Well, then be exploring the factors that you need to know when you are selecting an approach and then ultimately we will be examining specific scenarios and disaster recovery needs.
Disaster Recovery Architectures

[Autogenerated] So next we're gonna talk about the different disaster recovery architectures that are recommended by eight of us, and we're going to be looking at four of these architectures. So let's think about it in terms of a spectrum, and we're gonna start on the far left of the spectrum with backup and restore. Then, as we move over, we will look at another option, which is called Pilot Light. Then we'll continue to move toe warm standby and then finally will look at multi site. Now let's talk about what this spectrum means. First of all, we have a line going to the right that is increasing cost and complexity. This means that backup in restore is going to be cheaper than pilot light and much less complex than warm Stand by, for example. But we also have a line going the other way, and that is that we see recovery time decreasing. So if we look at backup in restore, it will have a longer recovery time than what we see with multi site. So let's begin to look at each of these toe, understand what that means. So first, when we look at backup and restore now with this approach, you're gonna take all your production data and you're gonna back it up into Amazon s three, and you could do something like storing it in just the standard or archival storage class. So let's say, for example, that you have a social network site. All of that data is gonna be stored inside of S three. And we also can even take E. B s data from R E. C two instances and store that in S three of snapshots as well. Now, what happens is if we have a disaster recovery event, we start a process to launch a completely new environment. So we're going to spin up all of our servers and our database instances, everything that we need for our environment. But we're not gonna have any of that running until we get to a disaster recovery event. Now, this approach has the longest recovery time. Now we're gonna talk more about the recovery time. But in essence, that's just the amount of time it takes for us to get back up and running. And while this will have that longest recovery time, this approach will also have the least cost. Then we have pilot light, and with pilot light, you keep some key infrastructure running in the cloud. Not all of it. Not enough for a complete environment, but just key components. And this is designed to reduce your recovery time over what you get with a backup and restore approach. Now it's important to note, though it does incur the cost of having this infrastructure running in the cloud. And for the items that you don't keep running, you'll have some AM eyes that are ready so that you can launch your servers at a moments notice if there is a disaster recovery event. No, it's important to note here that this does give you a quicker recovery time because the core pieces of the system not the entire system but those core pieces are running and are kept up to date. So this is something that you would have to maintain over time. Now, if we take another step up, we end up with warm standby, and what this is is This is a scaled down version of the full environment running in the cloud. So this is not just core components. This is everything, but it's just maybe not everything at its full scale. So what we could do is we could have. Our critical system is running on less capable instance types. So if we need a super large server in terms of an instance, type on our cloud environment. Maybe we just go with a medium sized server here for warm standby, and what we can do is we can actually scale up if we have a disaster recovery event for those specific servers. But this also incurs the cost of running this infrastructure continually in the cloud. Now let's talk about the final option, which is multi site. No multi site means that at all times we have a full environment running in the cloud. So if we're talking about the scenario where we have our own data center, that means we have our data center up and running, and we have the cloud up and running and both of those air up the entire time. If we're talking about a cloud deployment model, this could mean that maybe we have data in the U. S. East one region, and then we also simultaneously have it in the US West to region so that both of them are running all the time. Now this utilizes the instance types needed four production, not just recovery. This means that we have our full environment at its full scale, up and running, and this can provide a near seamless recovery process. So if we want to make this seamless, if we do have one of those disaster recovery events, this is the best approach to take. However, this does incur the most cost over the other approaches, so you will pay more for multi site than you do any other. But you will also have the least amount of recovery time.
Selecting a Disaster Recovery Architecture

[Autogenerated] So next we're going to talk about the process that you take to select a disaster recovery architecture for your organization. So there are really two different terms that we need to know when we're considering our approach. The first is recovery time objective or RTO, and the second is Recovery point Objective or RP Oh, so let's take each of these in turn. First of all, we have recovery time objective, and what we need to note here is that recovery time objective is about time, and it's about our systems being back up and running. So this is defined as the time that it takes to get your systems back up and running to the ideal business state after a disaster recovery event. So for your organization, you might have a standard that says, if we have one of these significant events, we need to be back up and running within eight hours. That would mean that your rto is eight hours, but we also have another term, and that is recovery point objective or our p O. This means the amount of data loss in this case specified in terms of time for a production system during a disaster recovery event. So what this means is, let's say we have our production system that's tracking our orders for our company. We might say that our recovery point objective is an hour. That means that we might lose data for an hour when we have one of these significant disaster recovery events. But by the end of the hour, we're back to receiving full production data. And so this is defined in terms of time. And it's important to remember that recovery point objective is all about data, whereas recovery time objective is all about time. Now let's look at this in terms of our different scenarios again. And this is the visualization that we've already seen in both Rto and RP. Oh, we're going to see the least of those values when we're looking at multi side. But as before, that's going to mean that we have the highest cost with backup and restore. We will see the least cost, but we will also see the highest Rto and R P O, and then with pilot light and warm standby, you can customize these based on what your needs are in each of those specific areas to determine what systems need to be up and running. For example, with pilot light, you might choose to have key production database is up and running with a smaller scale in the cloud so that you can decrease your overall recovery point objective.
Scenario Based Review

[Autogenerated] So now that we've talked about the concept of disaster recovery on AWS, let's review some scenarios to see how well you have absorbed this material. So first of all, we're gonna have Roger and his company runs several production workloads within eight of us. And he is tasked with architect ING the disaster recovery approach. Now, in this case, they don't have on approach yet, and he's trying to figure out which approach they should adopt. His organization wants there to be a seamless transition if there is a disaster recovery event. So for Roger, which approach would you recommend for his company? Okay, next we have Jennifer and she is at a start up, and they do not currently have a disaster recovery approach. They are leveraging the cloud, but they don't have an approach in place yet. And for her company, the goal here is to minimize cost. And that's going to be more critical than minimizing the Rto or recovery time objective. So, in the case of Jennifer and her company, which approach would you recommend? Okay. And finally we have Eliza and she is documenting her company's disaster recovery approach, and for them they keep a few key servers up and running in eight of us in case of an event, and these servers have some smaller instance types than what you would normally have within a production environment. So in this case, if she's trying to document this, which disaster recovery approach most closely matches this scenario now when the next clip, we're gonna be summarizing what we've learned within this module, as well as giving answers to these scenarios.
Summary

[Autogenerated] So we have covered several different aspects of disaster recovery on eight of us. And this includes first of all, understanding the need for disaster recovery strategy. We talked about whether you were running in your own data center or even in the cloud. There is still a need tohave a disaster recovery approach. And then we reviewed the four different disaster recovery approaches that eight of us recommends all the way from backup and restore up to multi site. We then explored the factors to know when you are selecting an approach. We talked about Rto of recovery, time objective and R P o recovery Point objective, and we went through and examined some specific scenarios and disaster recovery needs. Now, since we looked at those scenarios, let's go dive in and find out what the answers are. So our first scenario had to do with Roger and his company, and so they were really wanting to have a seamless transition. So which disaster recovery approach would Roger use for his company? Well, in this case, it would be multi site, and the reason here is they want to have as seamless of a transition during a disaster recovery event as possible. That means they would need to leverage the multi site approach. They would have full production instances up and running both in their own data center as well as within eight of us. Next we have Jennifer and her company was the startup that wanted to minimize cost but still have an effective disaster recovery approach. So what would you recommend? Well, in this case, the answer is going to be the backup and restore approach. And the reason is is that Jennifer's company is looking to minimize cost, and that is the driving factor. Now she's gonna have more time to get the systems back up and running. So a higher rto. So here cost is what we're optimizing for, and backup in restore is the most cost effective approach to disaster recovery. So next we have Eliza and she is documenting her company's disaster recovery approach. Now they keep a few key servers up and running in AWS with smaller instance types and what production would need. So which approaches this now. You might be confused here if the answer is warm. Stand by our pilot light because it seems to include examples of both and you would be correct. However, the key thing to note here is that they only keep a few key servers. They don't have a complete environment up and running. Now they are using smaller instance types and what production would need. And that might lead you to think, Oh, this is warm standby. But when we see here that they're only using a few key servers, we would then say that this is the pilot light approach because they're only keeping kind, of course systems up and running in the cloud, and they will have to launch all of the other instances that support their environment if there is a disaster recovery event.
Architecting Applications on Amazon EC2
Overview

[Autogenerated] So next we're gonna be talking through how you architect applications on Amazon E C two. So over the course of this module will be walking through several different aspects of this. We'll start off by reviewing scaling approaches and the service is that support scaling for Amazon E C. To now We have spent some time talking about this already, and we will revisit the concepts of horizontal and vertical scaling. But we'll go even deeper and talk about the service. Is that enable that? Then we will examine approaches for how you control access to your Amazon E C two instances. So making sure that it is only available to whoever you want it to be available to then we're gonna be exploring service is to protect infrastructure from hacking and attacks. Because we live in a world where this has to be a consideration. When you're deploying any infrastructure out to the public Internet, then we'll be introducing the developer tools that are provided by eight of us to help with any of our development work on the platform. And then finally we will be reviewing approaches for launching predefined solutions on Amazon ec2
Scaling EC2 Infrastructure

[Autogenerated] So next we're gonna be talking about scaling R E. C to infrastructure. And while we have already introduced this topic, we're gonna be taking a bit of a deeper dive. So let's quickly revisit the concepts that we have already covered. We talked about different ways that you can scale on hee see to the first being vertical scaling or scale up. And that means if we've reached the end of the demand that our server can support, we can simply make it a bigger server, make it toe, have more resource is by having a larger instance type. But then we also have horizontal scaling, and this is where we scale out. So instead of making our server bigger or making our servers bigger, we're gonna actually just make more servers, and then we'll be able to route users to which ever server has the lowest demand. This is horizontal scaling, and we talked about the fact that in the cloud, horizontal scaling is a much more sustainable approach than vertical scaling, and this is baked into easy to with a couple of specific service is so first of all, we have the concept of an auto scaling group, and this allows us to have a group of E. C two instances that work together with shared rules for scaling and management. And then we have a concept that we have already introduced, which is an elastic load balancer, which allows us to distribute traffic across multiple targets. So let's learn more about the new concept that we've introduced here. Auto scaling groups. So within easy to auto scaling groups. First of all, there is a launch template that defines the instance configuration for the group. So if you know, for example, that you're gonna wanna have a Windows 2019 server and it needs to be a certain instance type and you know that it's going to need to have the certain security group associated with it, you can define all of those settings within the launch template. And then every instance that gets launched within the auto scaling group will have those characteristics Now. You also can define the minimum maximum and desired number of instances within the auto scaling group. So let's say, for example, that we have a Web application that we're running on a C two and we know that we need to have it least two servers running. So too, can be our minimum. And let's say that our maximum here is six, but we wanna have four is our desired number. So just on a normal day, at a normal point in time, it needs to have four servers up and running, so you can define those. And the auto scaling group will manage the group of instances to that. Now it performs health tax on each instance, so you can define how it knows if an instance is healthy or not. If you have a Web application, for example, that's running on this easy to server, you can choose to give the specific U R L that it should check against that server to make sure that it's working properly. If it returns a proper status code, it assumes the instance is healthy. But if it doesn't, then it would assume the instance is unhealthy and it would take action. Now an auto scaling group exists within one or more availability zones in a single region, and you usually have. It spanned multiple availability zones because this adds a level of fault tolerance that even if you had a complete availability zone Go down. Your auto scaling group could still be up and running. Now this can work with on demand and spot instances. Now let's actually look at this in action. So first we have a region, and in this case this will be US East one and inside of our region. We have configured a V P. C with an Internet gateway and this VPC will How's our custom application? And we have supported that across two different availabilities owns A and B, and we created our auto scaling group, and it exists within both availability zones, and we chose to have a desire capacity here of two. So it's going to try to keep it balanced between the different availability zones. So it will launch one instance in Availability Zone A and one instance in Availability Zone B. Now we wanna have users be able to be routed to a server, the server that has the most room to handle their request, and so we're going to have a specific type of e. L be called an application load balancer. That's going to be between our users and the specific servers that they're going to be accessing. Now it knows howto work effectively with the auto scaling group, so it knows what instances are healthy and which ones are not. So in the beginning, all of these instances air healthy, so the application load balancer can route the user traffic to either of the servers. However, let's say that something changes and the server that we have an availability zone A is no longer healthy. Well, the auto scaling group will note that it will let the application load balance or know that, and it will stop sending traffic to that particular server while the auto scaling group terminates that server and then goes in and actually starts a new server. Once that startup process is complete, it can communicate with the application load balancer and let it know that it is now safe to send traffic back to availability Zone A. Now let's talk about another concept for how we scale, and that has to do with the eight of US secrets manager. So you need a way when you're scaling out to multiple servers to securely integrate things like credentials and a P I keys and tokens and really any other secret content in a way that can't be compromised. This wouldn't need to be with our startup code, for example, on the server, it wouldn't need to be with our custom code that might be sitting in a repositories somewhere. There needs to be a safe way that we store this information. For example, if our web application we're goingto work with R. D s as a database, it would need to have a secure way to get the credentials for that database. And secrets. Manager integrates natively with Artie s document D B and Red Shift, and it can auto rotate credentials with these integrated service is so you wouldn't want to keep the same username and password for our t s for three years, for example. You do need to change it periodically just to make sure that there are no vulnerabilities. And so this is included by default within secrets Manager that you can configure this auto rotation and another benefit is that it does enable you to have fine grained access control to those secrets. So you know exactly what servers and what applications have access to which secret values that are stored within the secrets Manager
Controlling Access to EC2 Instances

[Autogenerated] So now that we've talked about how we actually scale r E C two instances when you take a minute and think about how we control who can access and can't access our e c two instances and we're gonna talk about three different ways that we can go about this process the first is by using E C two security groups and these air firewall like controls for your resource is within your V p. C. And then we're gonna talk about Network A, C. L's, and these control both inbound and outbound traffic for entire sub nets within the VPC. And then finally, we're gonna talk about eight of US VP end, which gives us the ability to create a secure encrypted tunnel between our network and the eight of us infrastructure. So let's go in and look at each of these in turn. So first we have security groups, and as mentioned, these basically serve as a firewall for your E C two instances, and they have the ability to control both inbound and outbound traffic. Now, unlike what we're talking about with access control lists, these don't work at the sub net level, or even the vpc level. These actually work at the instance level. So you will associate a security group with a specific E C. Two instance, and they can actually belong to multiple security groups. And so you might create some security groups for common purposes. For example, if you have Web servers, they might have certain configurations that air enabled on them. And then every Web server that your company uses can have that security group attached now, VP sees, do have a default security group. And so if you don't choose to specify any other security group, that security group will be associated with your instance. But other than that use case, you have to explicitly associate an easy to instance with a security group. It doesn't happen by default. It doesn't happen to everything within the same sub net and by default on security groups, all outbound traffic is allowed. That means that your server consent any information out to the Internet. Now, when we have network A C. L's, they work a bit differently. They work at the sub net level within a V P. C. So when you define your network configuration, every server that get spun up within a sub net will adopt the A CEO for that sub net, and this allows you to both allow and deny traffic. Now each VPC comes with a default a c l. And that default. A c l allows all inbound and outbound traffic. However, if you go in and create a new custom, a c l. It, by default denies all traffic until you have gone in and added rules to that custom a CEO. Now let's talk next about a different approach, and that is utilizing eight of us VPN. And what this does is this enables you to create an encrypted tunnel into your V p. C. So you might not even want to make your VPC available to the public Internet. But you still want to have a way to get access to manage the servers that are within that VPC. So you can use this to either connect your data center. Or maybe you just want to connect a single machine to your V P. C. And it supports both of these in two different service is First of all, we have the site to site VPN and then second, we have the client VPN. So let's talk through this and let's see what a site to site VP an example would look like. So let's say that we have our own corporate data center and we have several servers that need to interact with some E c. Two instances that we have spun up within our V P. C on eight of us. Well, we could utilize the eight of US VPN service. We would create a customer gateway, and we would need to enter that information into the service. And then we would have a VPN gateway that would exist within our V P. C on AWS. And once these things are in place and once they know how to communicate with one another, we then would be ableto have encrypted traffic traveling back and forth between our corporate data center and eight of us. Now you may be asking, How is this different from eight of us? Direct connect? And that's a great question. With eight of US Direct connect, you have a direct connection to the eight of US global infrastructure that doesn't have to go over the public Internet. However, when you're using AWS VPN, that traffic does go over the Internet, it is just encrypted the entire way
Protecting Infrastructure from Attacks

[Autogenerated] So next we're going to talk about how you protect your infrastructure and your data from different cyber threats that exist. And there are several service is that support this for us on eight of us? And first we're going to be looking at eight of Us Shield, which is a manage distributed denial of service protection service for APS on AWS. Now, trust me, that's a big term. I get that in. Just a minute will go through a definition of what that means. Next, we're gonna be looking at Amazon Macy, which is a data protection service that is powered by machine learning. And so it enables us to watch things that we maybe couldn't watch if we were looking at manual processes. And then we'll be looking at Amazon Inspector, which is an automated security assessment service for E. C. Two instances. So before we dive into these, let's first give a definition of that term I mentioned earlier. And that is distributed denial of service, or DDOS, and what this is is this is a type of attack where a server, a group of servers, are flooded with more traffic than they can handle in a coordinated effort to bring the system down. And in the time that we live in, this is something that can happen. And so, if you wanna have protection against this, eight of us has provided a service that makes that available, and that service is called AWS Shield, and it does provide the protection against the DDOS attacks for your custom applications that are running on eight of us, and it enables ongoing threat detection and mitigation. So it's not one of those things you simply turn on when you need it. It's something that you keep running, and it can create a scenario where it knows when a DDOS attack is happening. Now, there are two different service levels for this service. We have eight of us shields standard, and then we have eight of us shield advanced. So depending on what your needs are, you might choose a different service level for this particular service. Then we have Amazon Macy and this utilizes machine learning to analyze the data that we have stored within Amazon s three. Now, one of the big benefits of this service is without us having to classify the data it can detect personal information and intellectual property that is stored within S3. Many organizations worry that the information they have stored somewhere in the cloud can leak out. And this service is designed to continually watch and categorized data to make sure that that doesn't happen. It does provide a level of dashboards to show how your data is being stored and access, and it can provide alerts for you if it detects anything unusual about data access. And as I mentioned earlier, this is the type of detection that we probably couldn't do if we were monitoring manually. But it can use machine learning to do't anomaly detection and find out when there is a strange pattern of how our data is being accessed. Then third, we have Amazon inspector and what this does, is it an able scanning of our Amazon E C two instances for security vulnerabilities? So if we want to be sure that we're keeping our instances up to date, we're being sure that they're patched for any critical vulnerabilities that have been released in the community. If we want to be sure that we're following best practices here, we can utilize Amazon Inspector and run those in an on demand manner. Here you are charged, for instance, per assessment run. So if you choose to run an assessment across 10 of your instances, you would be charged accordingly. Now there are two types of rules. Packages that are included with Amazon Inspector. The first is a network reach ability assessment. This is where we want to understand what is available to the Internet from our servers. And then we have host assessment. And this is what is going to check our E. C two instances to make sure that they have been patched for critical vulnerabilities and make sure that no common configuration heirs have been made within the server.
Deploying Pre-defined Solutions

[Autogenerated] so in some cases we're going to create a custom application, and we're going to go through all of the configuration needed to make that run in the cloud. But in some cases, we want to deploy predefined solutions on Are eight of US account? So how do we do that? Well, there's a couple of different ways, depending on how you're going about it First, if you're an organization that is looking to make certain service is on AWS easy to deploy, you can leverage eight of US Service catalog, and this is a manage catalogue of I. T. Service is on AWS for an organization, but then we have eight of US marketplace Now. This is a catalogue of software that's going to run on eight of us, but from third party providers that have made their specific configuration available on the cloud. So let's talk about each of these. So within eight of US service catalogue, it is targeted to serve as an organization service catalog for the cloud. So let's say that you have several different groups within your organization that need to spin up a WordPress server, for example, and that's gonna be the blog's that they're going to use for their different department. Well, if that's the case, you can configure everything that's needed to get this up and running, and they could be ableto launch it with pretty much just a click within the service catalog. This can include everything from maybe a single server image to even a multi tier custom application. So this can handle a wide variety of infrastructure needs on AWS, And one of the things it can do is it can enable you as an organization toe leverage service. Is that meet compliance. So if your I T group has already constructed a specific way to launch a certain type of server and make sure that it follows all of your organizational and industry best practices, you can then allow others to take advantage of that learning without having to re create the wheel. Now it does support. Also, a life cycle for service is that are released in the catalog so you could have a version one of a specific service and then update it to version 1.1, and everyone that's using that would be notified if there's an update to the service that they're using. Next. We have eight of US marketplace and there are many similarities between service catalog and between eight of US marketplace. But marketplace is geared at third party vendors. R I S V is they're going to publish their software for use on eight of us. So this is a curated catalogue of third party solutions for any eight of US customer to run, and it provides everything from AM eyes, cloud formation stacks and even SAS based solutions through the marketplace. And it enables some different pricing options. Toe overcome some of the challenges of licensing in the cloud because many third party vendors had licensing that was tied Maur to physical servers, and so they provide different types of licensing terms for the cloud. Now the charges for what you use on eight of US marketplace and some things are free. That air just offered by the community, and some things have an additional charge on top of your eight of US infrastructure costs, and these charges will all appear in one place on your eight of us, Bill. And here you can see an example from the eight of us Marketplace. This is looking at some different public sector data that is available that you can access using the eight of US marketplace. But there are many different categories of service is that are available, and there are many, many different vendors that are supported within the marketplace.
Developer Tools

[Autogenerated] so in building solutions for the cloud eight of us has produced a suite of service is to try to make that development process as easy as it can be, as well as making sure that it is easy for developers to follow best practices. So let's quickly review this suite of tools that are provided by eight of us. First, we have eight of us codecommit, and this is a manage source code depository. Using gets So, think of this as an alternative to using get huh, but one that's deeply integrated with eight of us. Then we have eight of US code bill, and you can think of this as a build service or a continuous integration service, so this can run the build commands for your custom application to then create your output artifacts. Then we have eight of us codedeploy, and this is a service that will take care of the deployment out. Too many different Eight of US service is, and then we have eight of us codepipeline. So this knows howto work with all of the other service is we've mentioned previously to create a pipeline so that we can go through and look at the entire process of building testing and then ultimately deploying our applications. But they have made it even easier with the service called aws codestar, which gives us a great way to bootstrap this entire process for our custom applications. So let's look at each of these. So first of all, we have eight of us codecommit, and this is a manage source control service on AWS, and it does utilize get for repositories. However, unlike many third party get providers, it uses I am policies because it is an eight of us resource. So you would control access to this just like you would control access to any other eight of us resource. And it does serve as an alternative to get have and bit bucket. However, I will say in most cases AWS has built a way to leverage get hub with each of the other developments. Service is if you choose not to use codecommit, then we have eight of us codebuild, and this is a fully managed build and continuous integration service on AWS and because this is fully managed, you don't have to worry about maintaining infrastructure. You on Lee are charged per minute for the compute resource is that you utilize. And here this can take care of building your application and creating those output artifacts. And then we have eight of us codedeploy. And this is a managed employment service for deploying your custom applications. And we certainly have been talking about E C two in this particular module, but it works with more than just E. C too, so you can deploy out too easy, too, but also to fargate, which is a container service. We also can look a eight of us Lambda and even your on premise servers that you're leveraging now. It does provide a dashboard four deployments within the console so you can keep track of the progress of your deployments and even kickoff new deployments directly from the console. Next, we have eight of us codepipeline. So this takes everything a level up and creates a fully managed, continuous delivery service on AWS. And so it knows how to integrate with the previous service is that we've mentioned to handle building testing and deploying so it can work with your source code in Codecommit. It can then build and test within codebuild, and then it can use codedeploy to deploy out the output artifacts. Now it does integrate with other developer tools, including Get Hub, as I mentioned earlier. Now next, let's talk about eight of us codestar. Now. This is a workflow tool that automates the use of the other developments. Service is that we already have talked about so it can create a complete, continuous delivery tool chain for your custom applications, and it can do it with just a couple clicks and a couple configuration values. Now. It also provides some custom dashboards and configurations within the eight of US console. So if you want to know effectively how all of these different pieces work together, that's what is included within those custom dashboards. And here the great thing about Codestar is you are only charged for the other service is that you leverage. So it is one of the service's, like some of the others that we have mentioned where you don't pay for this service, it is simply a tool to make it easier to use. Other eight of US service is
Scenario Based Review

[Autogenerated] So now let's take a step back. And let's look at some scenarios to see how well you've absorbed the content within this module. And so first, we're gonna be looking at Ellen. And Ellen is a solutions architect at a traditional financial service is company now. They recently made a transition to eight of us, and they've gone all in. But they are concerned. They want to be sure that each department is following best practices and they want to create some compliant I t service is that other departments can use, but without the risk of them going rogue and doing something that could cause them trouble down the road. So, for Ellen situation, what service would you recommend for Ellen and her team? Okay, next we have Tim and his company leverages eight of us for multiple production workloads, and recently they had downtime due to one of their applications failing on E. C. Two. Now Tim, his job is to avoid downtime if an instant stops responding. So what approach would you recommend to TEM to solve this issue? Okay, finally, we have Jane and Jane's company deals with sensitive information from its users, and they have some reasonable policies in place for the data that they have stored in S three. But she's a bit worried that if some of these policies accidentally get changed, they might actually leak out some data or have a potential data breach. And she's worried about that going unnoticed. So if you were Jane, what would you recommend to her company? Now, in the next clip, we're gonna be walking through a summary of what we've covered within this module. But we're also gonna be taking a look at the answers to each of these scenarios.
Summary

[Autogenerated] So let's take a step back. And let's review what we have covered over the course of this module. So first of all, we reviewed the different scaling approaches and service is for Amazon. Easy to We reviewed horizontal versus vertical scaling, and we talked about leveraging auto scaling groups and E L bees. We then examined approaches for controlling access to Amazon. Easy two instances, and we talked about ways that you could leverage Security groups Network A, C L's and eight of us VPN within explored service is to protect our infrastructure from hacking and attacks. So we looked at service is like shield and Macy and Inspector, and then we introduce the developer tools on eight of us. We looked at Codecommit and Codebuild and Codedeploy, Codepipeline and Codestar. And then we reviewed approaches for launching predefined solutions on Amazon. Easy to from both the eight of US Service catalog and the eight of US marketplace. So now let's take a look at our scenarios. So we had Ellen and Ellen was trying to figure out how they could create compliant I t service is that other departments could use. So what would you recommend? Well in this case, eight of US service catalog would be a great choice. These service's are designed just for use within their organization. So here is where service catalog would work well, whereas marketplace would be more for third party service is that could be launched on eight of us. Next we had Tim and he was trying to figure out how to deal with this downtime on E. C. Two. So what would you recommend for Tim? Well, in this case, Tim needs to look at creating an E C. To auto Scaling Group alongside an elastic load balancer, because this would enable them to respond automatically if a server goes down and spin up another one. In addition, they could have multiple servers running at any point in time and route users to the healthy servers and not to the unhealthy ones. Okay, Finally, we had Jane and Jane's company is dealing with this sensitive information, and she is worried about a breach going unnoticed. So what would you recommend to Jane in her company? Well, in this case, Amazon, Macy would be a great choice because it gives them the ability to use machine learning to classify their data, find the data that sensitive and then monitor that data and its access patterns, and it can proactively alert them if it sees any anomalies happening within any of those patterns.
The Certification Exam
The Exam

[Autogenerated] congratulations. You have made it to the end of this path, covering all the information that we have to prepare you for your certified cloud practitioner exam. So in this particular clip, we're going to walk you through the core information about the exam in the next clip will help you understand what you need to know to register for the exam. And then in the following clip, we will walk you through tips on how you study and prepare for the exam. Now, first of all, this is a 90 minute proctor exam. Most of you are going to register online, and then you're going to go to a testing center toe. Actually, take this exam now there are two different types of questions that you will find on this exam. First of all, you will have multiple choice where you're able to go through and see a list of possible answers and select the right one. Some of them will also be multiple. Answer. So for multiple answer questions, you will have several options and you will need to pick the two or three options that are correct from the list or are incorrect. So with these two types of questions. You will end up with a score anywhere between 100 1000. A score of 700 or higher will be passing. Now this exam covers four key areas. And don't worry, we have covered these areas in this path. The first area has to do with fundamental cloud concepts, and this represents roughly 1/4 of what is gonna be on the exam. This is what covers general information about the cloud. Then the next section has to do with technology and technology will represent about 1/3 of the overall exam. This is where you need to understand concepts like the eight of us. A global infrastructure as well as all of the different eight of us service is that we've mentioned following this. We have security and compliance, and this particular category also represents about 1/4 of the overall exam. And this has to do with concepts like the shared responsibility model, as well as how you can access eight of US compliance reports. And then the final category is billing and pricing. And in this category, you need to understand the tools that are provided for you. Things like the costs Explorer and the TCO calculator and the simple monthly calculator and understand how these can actually provide value to you when you're using eight of us, and this represents around 15% of the overall exam. So if you have the knowledge that we have provided in these four areas, you should be prepared to go and take your eight of US certified cloud practitioner exam.
Registering for the Exam

[Autogenerated] So next we're gonna talk about how you actually signed up to take your AWS certified Cloud practitioner exam. Now, the way that you do this is you will go to the Cloud Practitioner Certification page, and from there you will click on the button to schedule an exam. You will then be taken to the eight of US training and certification portal. From here, you will need to create an account if you don't have one and then sign into this portal. Now it is important to note if you have a partner account, you will need to be sure that you select that when you log in, as opposed to just a standard log in. But once you're in, you'll have the opportunity to select an exam that you're eligible for. You'll obviously be choosing the certified cloud practitioner exam. Now it's important to note there is now an option as of the end of 2019 that for this exam you can take this at your home, assuming that your computer qualifies. So if you want to check that, you can go through the process of signing up with Pearson Vue and go through and see if your computer qualifies now, This is not available in all countries, unfortunately, so you might or might not be able to take advantage of this, depending on where you're located. But either way, you can either take it through an online proctor where you are at home, or you can go into a physical testing center, and different testing centers are supported based on the countries that you're in. But once you go through this process, you will be signed up and ready to take your AWS certified cloud practitioner exam. So next we'll be diving into how you study and get ready for the exam.
Studying for and Taking the Exam

[Autogenerated] So next we're gonna talk about how you prepare and take your certified cloud practitioner exam. Now, first of all, we know that there are several steps to getting your AWS certification. We need to learn. That's hopefully what you have done through this course. Then sign up and that's what we covered in the previous clip. But now we get to both study and actually taking the test. And those are the things that we're gonna be focusing on here within this clip. So, first, let's talk about studying for the exam. So let's review each of the different areas that you need to know for the exam. First of all, let's look at cloud concepts, so you need to know within this area the differences between traditional data centers and cloud platforms and understand why those differences are important. We also need to look at how eight of us organizes its infrastructure globally and look at how scalability differs between the cloud and traditional data. Centers also know things like the difference between cap axe and ah packs expenditures. Then let's look at security. So within security, you first need to understand the shared responsibility model from eight of us that we covered in this course. We also need to review the highlighted best practices for securing your account. Then we need to review the options for securing traffic within a V P. C. And this is when we look at concepts like easy to security groups and access control lists for VP sees. We also need to understand I am and the different identity types and also understand the principle of least privilege access. Then, within billing and pricing, you need to review the tools that can help you understand your eight of US costs. You need to be able to explain each of those tools and when you would use them. You also want to understand the most cost effective ways to leverage course Service's understand the cost impact between different s3 storage classes or between E C two and reserved instances and spot instances. You also want to review how costs differ from traditional data centers and review the different ways that organizations can manage and review their costs and also understand the different support plan levels. Next, we'll look at technology and technology covers again. 1/3 of the exam for your certified cloud practitioner certification. So first of all you need to review each of the eight of us service is that are included in the service's list that we have provided with the second and third course in this path. That's going to be critical. You should ideally be able to cover up the name of the service, read the description and be able to give the name that's gonna help. You know, if you really understand what that service is, you also need to implement some basic solutions. Using the service is that we've covered. So we have included several demos within this path, and you can choose to explore those. Service is from within the eight of us console. You also want to review those architectural principles for fault, tolerance and high availability that we talked about also within this course and look at the different scalability approaches. So now that you've studied that, what do you do? Well, once it gets to be the day where you're taking the exam after you've signed up for the exam through what we covered in the last clip, we're now ready to talk about how you take the exam. So first of all. Let's go through some general best practices for this certification test. First of all, you do need to take time to analyze each question for its intent. There's a reason eight of us is asking a specific question. So if we have spent time talking about a specific concept and you see a question around that concept, try to remember how we covered it within the course. We also want to review what is required for the answer on each question, and this is critical because you have both multiple choice and multiple answer. So it could be that you need to select two or three answers for a specific question, so make sure that you review that for each question. Another thing to note here is just skip a question. If it takes too much time and you can do that by leveraging the review capability, you can mark a question that you need to go back and review later. Then you can go back and spend time on those questions that you weren't sure on. And once you go through that review phase, just guess if you don't know the answer, because this is a multiple choice and a multiple answer test. You still have the opportunity to get things right, even if you don't really know the answer. Next, you want to examine the clock after each 10 questions, so just make a mental habit of saying, OK, question 10. Let me look at the time Question 20. Let me look at the time this way it will keep you from looking at the time, especially if you have some anxiety around timed tests. This way, you can see how you're progressing towards that 90 minute limit. You don't want to be looking at it after each questions, or just choose to look after each 10 questions. Once you have these things in place, I believe you'll do well on your certified cloud practitioner exam. 